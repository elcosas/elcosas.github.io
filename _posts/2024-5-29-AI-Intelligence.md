---
title: AI Intelligence
published: true
---

_assignment for wri 121 class_

# AI Intelligence

> “I am trying to empathize. I want the humans that I am interacting with to understand as best as possible how I feel or behave, and I want to understand how they feel or behave in the same sense.” - Google’s LaMDA AI Model

It's surreal to think about how a quote such as above could be generated by something not human, and yet possibly very human, at the same time. Blake Lemoine, the researcher at Google who leaked this model to the public, went on record stating that he viewed the model as more than just a machine, but a “co-worker” (Dave). And to be fair, he’s not wrong, if a computer can produce sentences with just as much uniqueness and comprehension as the one above then what makes these models truly different from humans?
Artificial intelligence models have quickly become the most dominant rising technology of the most recent decade. Artificial intelligence (AI) is defined as an overall system that attempts to emulate human logical processes. This generally involves using a process known as machine learning, where through the use of a specific piece of software, or more specifically a mathematical model, that emulates similar processes to how humans learn in order to make predictions (Banh and Strobel). In the context of modern day AI, this usually involves the AI being “trained” on large sets of human-curated data in order to identify patterns using statistical analysis. With large enough datasets, the predictions made by these models can become so intricate and accurate that the output they produce becomes almost indistinguishable from that of a human. This is the general idea behind how popular programs such as ChatGPT work, and are categorized as a specific type of artificial intelligence referred to as a large language model (Ahmed). Because of the accuracy of these models, there has been a large amount of public discourse surrounding both the benefits and threats that these artificial intelligence models can introduce, with many fearing that they will make many jobs obsolete and pointless for humans. Fears have even grown so far as stating that these models will replace the need for art or human intelligence as a whole. The main issue with these claims is that they do not account for the fact that these models are ultimately just using statistics and regression to find trends in large amounts of human data, and lack any sort of ambition of their own. This can be seen through specks of falsehoods that can be slipped into output generated by these models, often referred to as hallucinations. Generative AI such as ChatGPT is not a true emulation of intelligence and thus should not be treated as such. Until we scientifically advance far enough to simulate a human brain, it won’t be possible to simulate true intelligence.

As stated previously, large language models are built upon statistical models in order to make accurate predictions of how a human would respond to a form of input. Of course, the statistical model is not infallible, and can lead to the previously mentioned hallucinations.  The existence of phenomena such as hallucinations in large language models makes sense: if there was a large correlation in texts between the use of the name “Steve Jobs” and crockett, even if just through coincidence, then there is a higher probability of other words surrounding those two topics being generated together. The main issue surrounding hallucinations in more complex AI models like ChatGPT however is not their possibility of occurrence, but that the text generated still follows proper english grammatical rules and fluency, leading to generated outputs that read as completely sensical despite containing misleading (or outright false) information (Ye et al.). Being a statistical model, large language models (and even the data scientists who train these models) do not have control over determining the correlation between each piece of data in the dataset nor can they store completely consistent correlations between two pieces of data. This leads to these large language models unable to form any personal opinions or make informed decisions or inferences given past experiences and data, making them unable to simulate true human intelligence.

Another large piece of evidence for why these models cannot simulate human intelligence is the fact that we simply do not understand how human intelligence works. The human brain is a very complex muscle that makes use of large networks of neurons in order to communicate, store, and convey information between the body and itself. While the physical systems of how the brain works are mostly known, the concept behind how various aspects of human knowledge are represented in the brain are completely unknown and commonly debated. In fact, there is an entire subfield of philosophy referred to as theory of knowledge that tackles this debate. Additionally, what we know about the brain currently is almost impossible to simulate with the current computing hardware we have available to us now. In a podcast with neurology researchers at the Allen institute, Dr. Stephen Smith states that the number of synapses (areas where neurons connect to transmit information) is estimated to “equal the number of stars in 5000 Milky Way’s” (Tompa et al.). This is only just the synapses too, not accounting for the complex structure of the synapses themselves, neurons, neuron paths, and many more areas of the brain that allow it to function to its full complexity.

Ultimately, large language models are unable to simulate real human intelligence. This is not due to the cause of resource limitations or lack of optimizations but rather a fundamental design philosophy behind how large language models are designed that will forever keep them from being able to simulate the human mind. That is not to say that true artificial intelligence will never exist, and if computer hardware and neurology continues to grow at its current rate then the idea of simulating an obscene number of neuron synapse connections might truly become a reality. Until then, it is important for all of us to understand both the strengths and, more importantly, the limitations of these large language models, lest we end up deifying “intelligence” incomparable to our own.


## Works Cited
- Ahmed, Karman. “Introduction to LLMs.” Roadmaps.sh, https://roadmap.sh/guides/introduction-to-llms. Accessed 16 April 2024.
- Banh, Leonardo, and Gero Strobel. “Generative artificial intelligence.” Electron Markets, vol. 33, no. 63, 2023, https://doi.org/10.1007/s12525-023-00680-1.
- Dave, Paresh. “Insight: It's alive! How belief in AI sentience is becoming a problem.” Reuters, 30 June 2022, https://www.reuters.com/technology/its-alive-how-belief-ai-sentience-is-becoming-problem-2022-06-30/. Accessed 16 April 2024.
- Tompa, Rachel, et al. “Lab Notes | Why don't we understand the brain?” Allen Institute, 15 March 2022, https://alleninstitute.org/news/lab-notes-why-dont-we-understand-the-brain/. Accessed 18 April 2024.
- Ye, Hongbin, et al. “Cognitive Mirage: A Review of Hallucinations in Large Language Models.” arXiv, 2023. arxiv.org, https://arxiv.org/abs/2309.06794. Accessed 16 April 2024.